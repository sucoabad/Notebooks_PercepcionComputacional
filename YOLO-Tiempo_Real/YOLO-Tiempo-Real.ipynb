{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO en Tiempo Real: Detección de Objetos\n",
    "### Percepción Computacional - Universidad de los Hemisferios\n",
    "#### Maestría en Inteligencia Artificial Aplicada\n",
    "\n",
    "Bienvenidos a este cuaderno de Jupyter, donde exploraremos la emocionante tarea de la detección de objetos en tiempo real utilizando YOLO (You Only Look Once), una técnica avanzada de visión por computadora. Esta experiencia de aprendizaje forma parte del programa de la Maestría en Inteligencia Artificial Aplicada, impartido por la Universidad de los Hemisferios.\n",
    "\n",
    "En este cuaderno, aprenderás a utilizar YOLO para identificar y localizar diversos objetos en imágenes y secuencias de video. Esta habilidad es esencial en una amplia gama de aplicaciones, desde la conducción autónoma hasta la vigilancia de seguridad, y representa uno de los avances más emocionantes en el campo de la inteligencia artificial y la visión por computadora.\n",
    "\n",
    "A lo largo de este curso, explorarás conceptos clave como:\n",
    "\n",
    "- Detección de objetos en tiempo real.\n",
    "- Configuración de modelos YOLO pre-entrenados.\n",
    "- Interpretación de resultados de detección.\n",
    "- Aplicaciones prácticas de la detección de objetos.\n",
    "\n",
    "¡Prepárate para sumergirte en un emocionante mundo de visión por computadora y adquirir habilidades valiosas que te servirán en tu carrera en inteligencia artificial y más allá!\n",
    "\n",
    "Recuerda consultar este cuaderno para acceder a los recursos y ejemplos que te ayudarán a comprender YOLO y la percepción computacional en tiempo real.\n",
    "\n",
    "Eugenio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código utiliza el comando `!pip` para instalar dos paquetes de Python: `opencv-python` y `ultralytics`.\n",
    "\n",
    "- `opencv-python`: Esta es una biblioteca popular para tareas de visión por computadora en Python. Proporciona una variedad de funciones para el procesamiento de imágenes y videos, incluyendo detección de objetos, manipulación de imágenes y extracción de características.\n",
    "\n",
    "- `ultralytics`: Esta es una biblioteca construida sobre PyTorch, que se enfoca principalmente en tareas de visión por computadora como detección de objetos y clasificación de imágenes. Proporciona interfaces fáciles de usar para entrenar y evaluar modelos de aprendizaje profundo para estas tareas.\n",
    "\n",
    "Al ejecutar este comando, estás instalando estos paquetes en tu entorno de Python para que puedas usarlos en tu código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.0.82)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.2.35)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (3.9.0)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (10.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.3.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (0.18.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\sucoa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías necesarias\n",
    "!pip install opencv-python ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código importa tres bibliotecas en Python:\n",
    "\n",
    "1. `ultralytics`: Esta es una biblioteca que proporciona una interfaz para usar modelos de detección de objetos YOLO (You Only Look Once). Permite entrenar, evaluar y utilizar modelos de detección de objetos de manera eficiente.\n",
    "\n",
    "2. `cv2` (OpenCV): Esta es una biblioteca popular para el procesamiento de imágenes y videos en Python. Proporciona una amplia gama de funciones para trabajar con imágenes y videos, incluyendo cargar imágenes, realizar operaciones de procesamiento de imágenes, y mostrar imágenes en una ventana, entre otros.\n",
    "\n",
    "3. `math`: Este es un módulo estándar de Python que proporciona funciones matemáticas comunes, como funciones trigonométricas, logarítmicas y aritméticas.\n",
    "\n",
    "El código importa estas bibliotecas para utilizarlas en el resto del programa, pero en este fragmento específico no se están utilizando ninguna función o clase de estas bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo YOLO pre-entrenado en COCO dataset\n",
    "model = YOLO(\"yolo-Weights/yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'person',\n",
       " 1: 'bicycle',\n",
       " 2: 'car',\n",
       " 3: 'motorcycle',\n",
       " 4: 'airplane',\n",
       " 5: 'bus',\n",
       " 6: 'train',\n",
       " 7: 'truck',\n",
       " 8: 'boat',\n",
       " 9: 'traffic light',\n",
       " 10: 'fire hydrant',\n",
       " 11: 'stop sign',\n",
       " 12: 'parking meter',\n",
       " 13: 'bench',\n",
       " 14: 'bird',\n",
       " 15: 'cat',\n",
       " 16: 'dog',\n",
       " 17: 'horse',\n",
       " 18: 'sheep',\n",
       " 19: 'cow',\n",
       " 20: 'elephant',\n",
       " 21: 'bear',\n",
       " 22: 'zebra',\n",
       " 23: 'giraffe',\n",
       " 24: 'backpack',\n",
       " 25: 'umbrella',\n",
       " 26: 'handbag',\n",
       " 27: 'tie',\n",
       " 28: 'suitcase',\n",
       " 29: 'frisbee',\n",
       " 30: 'skis',\n",
       " 31: 'snowboard',\n",
       " 32: 'sports ball',\n",
       " 33: 'kite',\n",
       " 34: 'baseball bat',\n",
       " 35: 'baseball glove',\n",
       " 36: 'skateboard',\n",
       " 37: 'surfboard',\n",
       " 38: 'tennis racket',\n",
       " 39: 'bottle',\n",
       " 40: 'wine glass',\n",
       " 41: 'cup',\n",
       " 42: 'fork',\n",
       " 43: 'knife',\n",
       " 44: 'spoon',\n",
       " 45: 'bowl',\n",
       " 46: 'banana',\n",
       " 47: 'apple',\n",
       " 48: 'sandwich',\n",
       " 49: 'orange',\n",
       " 50: 'broccoli',\n",
       " 51: 'carrot',\n",
       " 52: 'hot dog',\n",
       " 53: 'pizza',\n",
       " 54: 'donut',\n",
       " 55: 'cake',\n",
       " 56: 'chair',\n",
       " 57: 'couch',\n",
       " 58: 'potted plant',\n",
       " 59: 'bed',\n",
       " 60: 'dining table',\n",
       " 61: 'toilet',\n",
       " 62: 'tv',\n",
       " 63: 'laptop',\n",
       " 64: 'mouse',\n",
       " 65: 'remote',\n",
       " 66: 'keyboard',\n",
       " 67: 'cell phone',\n",
       " 68: 'microwave',\n",
       " 69: 'oven',\n",
       " 70: 'toaster',\n",
       " 71: 'sink',\n",
       " 72: 'refrigerator',\n",
       " 73: 'book',\n",
       " 74: 'clock',\n",
       " 75: 'vase',\n",
       " 76: 'scissors',\n",
       " 77: 'teddy bear',\n",
       " 78: 'hair drier',\n",
       " 79: 'toothbrush'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisamos las clases que puede detectar el modelo\n",
    "model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una lista de nombres con todas las clases para identificar objetos detectados\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"bear\", \"bottle\", \n",
    "              \"apple\", \"keyboard\", \"cell phone\", \"teddy bear\"\n",
    "              ] # Aquí se enumeran todas las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuramos la captura de video desde la cámara\n",
    "#captura = cv2.VideoCapture('http://192.168.100.3:4747/video') # Se abre la cámara por defecto\n",
    "captura = cv2.VideoCapture(0) # Se abre la cámara por defecto\n",
    "\n",
    "# Establecemos el ancho y alto de la imagen\n",
    "captura.set(cv2.CAP_PROP_FRAME_WIDTH, 640) # Ancho de la imagen\n",
    "captura.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) # Alto de la imagen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cv2.CAP_PROP_FRAME_WIDTH, 640) # Ancho de la imagen\n",
    "captura.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) # Alto de la imagen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  'source' is missing. Using 'source=C:\\Users\\sucoA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\assets'.\n",
      "\n",
      "image 1/2 C:\\Users\\sucoA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\assets\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 191.4ms\n",
      "Confidence ---> 0.88\n",
      "Class name --> bottle\n",
      "Confidence ---> 0.87\n",
      "Class name --> person\n",
      "Confidence ---> 0.86\n",
      "Class name --> person\n",
      "Confidence ---> 0.83\n",
      "Class name --> person\n",
      "Confidence ---> 0.27\n",
      "Class name --> person\n",
      "Confidence ---> 0.26\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Obtenemos el nombre de la clase detectada\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(box\u001b[38;5;241m.\u001b[39mcls[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass name -->\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mclassNames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Mostramos el nombre de la clase junto a la caja delimitadora\u001b[39;00m\n\u001b[0;32m     30\u001b[0m org \u001b[38;5;241m=\u001b[39m [x1, y1]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Iniciamos un bucle para procesar los fotogramas de la cámara\n",
    "while True:\n",
    "    success, img = captura.read() # Capturamos un fotograma\n",
    "\n",
    "    # Realizamos la detección de objetos en la imagen capturada (usando el modelo de YOLO pre-entrenado que cargamos anteriormente)\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "   # Procesamos los resultados de la detección\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "\n",
    "        # Iteramos sobre las cajas delimitadoras detectadas\n",
    "        for box in boxes:\n",
    "            # Obtenemos las coordenadas de la caja delimitadora\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # Convertimos a valores enteros\n",
    "\n",
    "            # Dibujamos la caja delimitadora en la imagen\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 1)\n",
    "\n",
    "            # Obtenemos la confianza de la detección\n",
    "            confidence = math.ceil((box.conf[0]*100))/100\n",
    "            print(\"Confidence --->\",confidence)\n",
    "\n",
    "            # Obtenemos el nombre de la clase detectada\n",
    "            cls = int(box.cls[0])\n",
    "            print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            # Mostramos el nombre de la clase junto a la caja delimitadora\n",
    "            org = [x1, y1]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            color = (255, 0, 0) # Color: Azul (formato BGR)\n",
    "            thickness = 1\n",
    "            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
    "\n",
    "    # Mostramos la imagen con las detecciones\n",
    "    cv2.imshow('Webcam', img)\n",
    "\n",
    "    # Salimos del bucle si se presiona la tecla 'q'\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "captura.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de índices de clases de interés\n",
    "clases_interes = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos la captura de video desde la cámara\n",
    "#captura = cv2.VideoCapture('http://192.168.100.3:4747/video') # Se abre la cámara por defecto\n",
    "captura = cv2.VideoCapture(0) # Se abre la cámara por defecto\n",
    "\n",
    "# Establecemos el ancho y alto de la imagen\n",
    "captura.set(cv2.CAP_PROP_FRAME_WIDTH, 640) # Ancho de la imagen\n",
    "captura.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) # Alto de la imagen\n",
    "# Iniciamos un bucle para procesar los fotogramas de la cámara\n",
    "while True:\n",
    "    success, img = captura.read() # Capturamos un fotograma\n",
    "\n",
    "    # Realizamos la detección de objetos en la imagen capturada (usando el modelo de YOLO pre-entrenado que cargamos anteriormente)\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # Procesamos los resultados de la detección\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "\n",
    "        # Iteramos sobre las cajas delimitadoras detectadas\n",
    "        for box in boxes:\n",
    "            # Obtenemos el nombre de la clase detectada\n",
    "            cls = int(box.cls[0])\n",
    "\n",
    "            # Solo procesamos la detección si la clase es de interés\n",
    "            if cls in clases_interes:\n",
    "                # Obtenemos las coordenadas de la caja delimitadora\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # Convertimos a valores enteros\n",
    "\n",
    "                # Dibujamos la caja delimitadora en la imagen\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 1)\n",
    "\n",
    "                # Obtenemos la confianza de la detección\n",
    "                confidence = math.ceil((box.conf[0]*100))/100\n",
    "                print(\"Confidence --->\",confidence)\n",
    "\n",
    "                print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "                # Mostramos el nombre de la clase junto a la caja delimitadora\n",
    "                org = [x1, y1]\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                fontScale = 1\n",
    "                color = (255, 0, 0) # Color: Azul (formato BGR)\n",
    "                thickness = 1\n",
    "                cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
    "\n",
    "    # Mostramos la imagen con las detecciones\n",
    "    cv2.imshow('Webcam', img)\n",
    "\n",
    "    # Salimos del bucle si se presiona la tecla 'q'\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberamos la cámara y cerramos todas las ventanas\n",
    "captura.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarea: Aplicación de detección de objetos específicos con YOLO**\n",
    "\n",
    "Objetivo: Modificar el script de detección de objetos en tiempo real con YOLO para que solo detecte ciertas clases de objetos. Luego, darle una aplicación específica a esta detección.\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "1. Elija un subconjunto de las 80 clases disponibles en YOLO que sean relevantes para una aplicación específica. Por ejemplo, si está creando una aplicación para ayudar a los conductores a detectar peatones y vehículos, podría elegir las clases 'persona', 'bicicleta', 'coche', 'moto', 'autobús' y 'camión'.\n",
    "\n",
    "2. Modifique el script de detección de objetos en tiempo real para que solo detecte las clases que ha seleccionado. Para hacer esto, necesitará crear una lista de los índices de las clases que ha selecciono y luego agregar una condición en el bucle donde se procesan los resultados de la detección para solo procesar las detecciones que corresponden a las clases seleccionadas.\n",
    "\n",
    "3. Pruebe su script modificado para asegurarse de que solo está detectando las clases que ha seleccionado.\n",
    "\n",
    "4. Piense en una aplicación específica para su detección de objetos. ¿Cómo podría usar la información de las detecciones para ayudar al usuario? Por ejemplo, en la aplicación para conductores, podría alertar al usuario si detecta un peatón o un vehículo en una zona de peligro.\n",
    "\n",
    "5. En clase, presente las clases que seleccionó, cómo modificó el script de detección de objetos, los resultados de sus pruebas, la aplicación específica que ha ideado para su detección de objetos, y cómo la aplicación serviría al usuario final u organización."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
